Ok, here's a breakdown of the `plan_and_execute_prompt_library_workflow` tool's actions, step by step, based on the provided code:

1.  **Initialization:**
    *   The tool receives repository information (`repo_description`, `main_languages`, `file_patterns`, `key_features`), the current `phase` of the workflow (defaulting to 1), and the `workflow_state`.
    *   If `workflow_state` is not provided or is invalid, it initializes a new workflow state. This state stores repository information, recommended/created/deployed rules, and workspace preparation status. It also immediately calls `prep_workspace()` and stores the result.

2.  **Phase Execution (Based on the `phase` input):**
    *   The tool uses a conditional structure (`if/elif/else`) to determine which phase of the workflow to execute.  Each `execute_phase_X` function encapsulates the logic for a specific phase.

3.  **`execute_phase_1` (Repository Analysis):**
    *   **Workspace Check:**  Verifies if the workspace is prepared (`workflow_state.get("workspace_prepared")`). If not, it calls `prep_workspace()` to set up directories and updates the workflow state.
    *   **Repo Analysis Prompt:** Calls `repo_analysis_prompt` with repository information to get an analysis and rule suggestions.
    *   **Results Processing:** Parses the analysis results from `repo_analysis_prompt`'s output into a structured format, extracting repository type, common patterns, and recommended rules.
    *   **State Update:** Updates the `workflow_state` with the analysis results and marks `phase_1_complete` as `True`.

4.  **`execute_phase_2` (Rule Identification):**
    *   **Prerequisites Check:** Checks if Phase 1 is complete.
    *   **Repository Summary Creation:** Creates a summary string from repository information to be used by the `recommend_cursor_rules` function.
    *   **Rule Recommendation:** Calls `recommend_cursor_rules` with the repository summary to get a list of recommended cursor rules.
    *   **Rule Processing:** Categorizes, prioritizes, and filters the recommended rules. It also attempts to identify dependencies between the rules.
    *   **State Update:** Updates the `workflow_state` with the recommended, categorized, and selected rules. Marks `phase_2_complete` as `True`.

5.  **`execute_phase_3` (Workspace Preparation):**
    *   **Prerequisites Check:** Checks if Phase 2 is complete.
    *   **Rule Name Processing:** Extracts rule names from the selected rules, makes them filename-safe, and ensures uniqueness.
    *   **Tool Execution:**
        *   Calls `ensure_makefile_task()` to ensure the Makefile has the `update-cursor-rules` task.
        *   Calls `update_dockerignore()` to exclude the cursor rules drafts directory from Docker builds.
        *   Calls `create_cursor_rule_files()` to create empty `.mdc.md` files for each rule in the `hack/drafts/cursor_rules` directory.
    *   **State Update:**  Updates the `workflow_state` with the rule file names and marks `phase_3_complete` as `True`.

6.  **`execute_phase_4` (Rule Creation):**
    *   **Prerequisites Check:** Checks if Phase 3 is complete.
    *   **Rule Processing:** Iterates through the rule file names and:
        *   Extracts rule metadata (description, file patterns, content patterns, etc.) from `rule_file_mapping`.
        *   Calls `generate_cursor_rule()` to generate the content of the cursor rule in Markdown format.
        *   Calls `save_cursor_rule()` to save the generated content to a `.mdc.md` file in the `hack/drafts/cursor_rules` directory.
    *   **State Update:** Updates the `workflow_state` with the created rules and any errors that occurred during rule creation. Marks `phase_4_complete` as `True` if any rules were successfully created.

7.  **`execute_phase_5` (Deployment and Testing):**
    *   **Prerequisites Check:** Checks if Phase 4 is complete.
    *   **Deployment:** Calls `run_update_cursor_rules()` to execute the `update-cursor-rules` task in the Makefile, which copies the cursor rule files to the `.cursor/rules` directory.
    *   **State Update:** Updates the `workflow_state` with the deployed rules and marks `phase_5_complete` as `True`.
    *   **Testing Instructions:** Provides instructions for testing the deployed cursor rules.

8.  **Return Value:**
    *   Each `execute_phase_X` function returns a dictionary containing the status of the phase, a message, a checklist of completed tasks, relevant data (created rules, errors, etc.), the updated `workflow_state`, the `next_phase` to execute, and any `next_steps` for the user. The `plan_and_execute_prompt_library_workflow` tool returns this dictionary to the caller.


# Tool calls

### get_static_cursor_rules

> Request:

```json
{"method":"tools/call","params":{"name":"get_static_cursor_rules","arguments":{"rule_names":["tree","repomix"]},"_meta":{"progressToken":0}}}
```

> Response:

```json
{"content":[{"type":"text","text":"{\"rules\": [{\"rule_name\": \"tree.mdc.md\", \"content\": \"---\\ndescription: Display repository structure\\nglobs: *\\nalwaysApply: false\\n---\\nWhen the user asks about the project layout, structure, or organization, run the command `tree -L 7 -I \\\"*.pyc|__pycache__|.git|.pytest_cache|.ruff_cache|.mypy_cache|.coverage|htmlcov|.venv|.env|*.egg-info|build|dist|node_modules|.DS_Store|images\\\"` to help understand and visualize the repository structure.\\n\"}, {\"rule_name\": \"repomix.mdc.md\", \"content\": \"---\\ndescription: Repomix tool\\nglobs: *\\nalwaysApply: false\\n---\\n# Repomix Project Layout\\n\\nRules for understanding and navigating the repomix project structure.\\n\\n<rule>\\nname: project_layout_guide\\ndescription: Guide to the repomix project structure and organization\\nfilters:\\n  # Match any file in the project\\n  - type: file_extension\\n    pattern: \\\".*\\\"\\n  # Match project initialization events\\n  - type: event\\n    pattern: \\\"file_create\\\"\\n\\nactions:\\n  - type: suggest\\n    message: |\\n      # Repomix Project Structure\\n\\n      This repository implements a tool for code repository summarization and packaging for LLM consumption.\\n\\n      ## Core Features\\n\\n      - **Repository Summarization:** Extracts and summarizes code repositories\\n      - **Code Packaging:** Prepares code for LLM consumption\\n      - **Security Checks:** Validates file safety and filters untrusted files\\n      - **Multiple Output Formats:** Supports markdown, plain text, and XML output styles\\n      - **CLI Interface:** Command-line interface for easy usage\\n      - **Web Interface:** Browser-based interface for repository processing\\n\\n      ## Directory Structure\\n\\n      ```\\n      .\\n      \\u251c\\u2500\\u2500 .cursor/                     # Active cursor rules directory\\n      \\u2502   \\u2514\\u2500\\u2500 rules/                   # Production cursor rules\\n      \\u251c\\u2500\\u2500 bin/                         # Binary executables\\n      \\u2502   \\u2514\\u2500\\u2500 repomix.cjs              # Main executable\\n      \\u251c\\u2500\\u2500 src/                         # Source code\\n      \\u2502   \\u251c\\u2500\\u2500 cli/                     # Command-line interface\\n      \\u2502   \\u2502   \\u251c\\u2500\\u2500 actions/             # CLI action implementations\\n      \\u2502   \\u2502   \\u251c\\u2500\\u2500 cliPrint.ts          # CLI output utilities\\n      \\u2502   \\u2502   \\u251c\\u2500\\u2500 cliRun.ts            # CLI execution logic\\n      \\u2502   \\u2502   \\u251c\\u2500\\u2500 cliSpinner.ts        # CLI progress indicators\\n      \\u2502   \\u2502   \\u2514\\u2500\\u2500 types.ts             # CLI type definitions\\n      \\u2502   \\u251c\\u2500\\u2500 config/                  # Configuration handling\\n      \\u2502   \\u2502   \\u251c\\u2500\\u2500 configLoad.ts        # Config loading utilities\\n      \\u2502   \\u2502   \\u251c\\u2500\\u2500 configSchema.ts      # Config validation schema\\n      \\u2502   \\u2502   \\u251c\\u2500\\u2500 defaultIgnore.ts     # Default ignore patterns\\n      \\u2502   \\u2502   \\u2514\\u2500\\u2500 globalDirectory.ts   # Global directory management\\n      \\u2502   \\u251c\\u2500\\u2500 core/                    # Core functionality\\n      \\u2502   \\u2502   \\u251c\\u2500\\u2500 file/                # File operations\\n      \\u2502   \\u2502   \\u251c\\u2500\\u2500 metrics/             # Metrics calculation\\n      \\u2502   \\u2502   \\u251c\\u2500\\u2500 output/              # Output generation\\n      \\u2502   \\u2502   \\u251c\\u2500\\u2500 packager/            # Output packaging\\n      \\u2502   \\u2502   \\u251c\\u2500\\u2500 security/            # Security validation\\n      \\u2502   \\u2502   \\u251c\\u2500\\u2500 tokenCount/          # Token counting utilities\\n      \\u2502   \\u2502   \\u2514\\u2500\\u2500 treeSitter/          # Code parsing with tree-sitter\\n      \\u2502   \\u251c\\u2500\\u2500 index.ts                 # Main entry point\\n      \\u2502   \\u2514\\u2500\\u2500 shared/                  # Shared utilities\\n      \\u2502       \\u251c\\u2500\\u2500 constants.ts         # Shared constants\\n      \\u2502       \\u251c\\u2500\\u2500 errorHandle.ts       # Error handling\\n      \\u2502       \\u251c\\u2500\\u2500 logger.ts            # Logging utilities\\n      \\u2502       \\u251c\\u2500\\u2500 processConcurrency.ts # Concurrency management\\n      \\u2502       \\u2514\\u2500\\u2500 types.ts             # Shared type definitions\\n      \\u251c\\u2500\\u2500 tests/                       # Test suites\\n      \\u2502   \\u251c\\u2500\\u2500 cli/                     # CLI tests\\n      \\u2502   \\u251c\\u2500\\u2500 config/                  # Configuration tests\\n      \\u2502   \\u251c\\u2500\\u2500 core/                    # Core functionality tests\\n      \\u2502   \\u251c\\u2500\\u2500 integration-tests/       # Integration tests\\n      \\u2502   \\u251c\\u2500\\u2500 shared/                  # Shared utility tests\\n      \\u2502   \\u2514\\u2500\\u2500 testing/                 # Test utilities\\n      \\u2514\\u2500\\u2500 website/                     # Web interface\\n          \\u251c\\u2500\\u2500 client/                  # Frontend application\\n          \\u2502   \\u251c\\u2500\\u2500 components/          # Vue components\\n          \\u2502   \\u2514\\u2500\\u2500 src/                 # Frontend source code\\n          \\u2514\\u2500\\u2500 server/                  # Backend server\\n              \\u2514\\u2500\\u2500 src/                 # Server source code\\n      ```\\n\\n      ## Primary Components\\n\\n      ### Source Code (`src/`)\\n      Core TypeScript application code organized into modules:\\n      - `cli/`: Command-line interface implementation\\n      - `config/`: Configuration management\\n      - `core/`: Core functionality including file processing, security, and output generation\\n      - `shared/`: Shared utilities and types\\n\\n      ### Tests (`tests/`)\\n      Comprehensive test suite including unit tests and integration tests for all components.\\n\\n      ### Website (`website/`)\\n      Web interface for the application with client and server components:\\n      - `client/`: Vue-based frontend application\\n      - `server/`: Backend API server\\n\\n      ### Binary (`bin/`)\\n      Executable files for running the application from the command line.\\n\\n      ## Development Workflow\\n\\n      ### Feature Development\\n      1. **Understand Requirements** - Review existing code and documentation\\n      2. **Implement Changes** - Add or modify code with appropriate tests\\n      3. **Test Thoroughly** - Run tests to ensure functionality\\n      4. **Document Changes** - Update documentation as needed\\n\\n      ### Testing\\n      - Run unit tests: `npm test`\\n      - Run specific tests: `npm test -- <test-pattern>`\\n      - Run integration tests: `npm run test:integration`\\n\\n      ### Building\\n      - Build the application: `npm run build`\\n      - Build the website: `cd website && npm run build`\\n\\nexamples:\\n  - input: |\\n      # I'm new to the project, where should I put my code for a new CLI command?\\n    output: |\\n      For a new CLI command, you should:\\n\\n      1. Create a new action file in `src/cli/actions/` (e.g., `myNewAction.ts`)\\n      2. Implement the action following the pattern of existing actions\\n      3. Add appropriate tests in `tests/cli/actions/`\\n      4. Update the CLI command registration in the main CLI runner\\n\\n      Look at existing action files like `defaultAction.ts` or `initAction.ts` for examples of the structure.\\n\\n  - input: |\\n      # How is the code parsing functionality organized?\\n    output: |\\n      The code parsing functionality is in the `src/core/treeSitter/` directory:\\n\\n      - `ext2Lang.ts`: Maps file extensions to language types\\n      - `lang2Query.ts`: Maps languages to appropriate queries\\n      - `languageParser.ts`: Core parsing functionality\\n      - `loadLanguage.ts`: Loads language grammars\\n      - `parseFile.ts`: Main file parsing entry point\\n      - `parseStrategies/`: Language-specific parsing strategies\\n      - `queries/`: Tree-sitter queries for different languages\\n\\n      This module uses tree-sitter to parse code files and extract meaningful information.\\n\\nmetadata:\\n  priority: high\\n  version: 1.0\\n  tags:\\n    - project-structure\\n    - organization\\n    - development-workflow\\n</rule>\\n\\n<rule>\\nname: project_standards\\ndescription: Standards for code quality and organization in the repomix project\\nfilters:\\n  # Match any file in the project\\n  - type: file_extension\\n    pattern: \\\".*\\\"\\n  # Match project initialization events\\n  - type: event\\n    pattern: \\\"file_create\\\"\\n\\nactions:\\n  - type: suggest\\n    message: |\\n      # Repomix Project Standards\\n\\n      ## TypeScript Standards\\n\\n      - **Code Style**: Follow the project's biome.json configuration\\n      - **Type Safety**: Strong typing for all functions and classes\\n      - **Documentation**: JSDoc comments for public APIs\\n      - **Testing**: Comprehensive test coverage with vitest\\n\\n      ## Project Structure Standards\\n\\n      - **Module Organization**: Functionality grouped by domain (cli, core, config)\\n      - **Component Isolation**: Clear separation of concerns between modules\\n      - **Worker Architecture**: CPU-intensive tasks use worker threads\\n      - **Error Handling**: Consistent error handling patterns\\n\\n      ## Web Interface Standards\\n\\n      - **Frontend**: Vue.js components with TypeScript\\n      - **Backend**: Node.js API with TypeScript\\n      - **Internationalization**: Support for multiple languages\\n      - **Responsive Design**: Mobile and desktop friendly UI\\n\\n      ## Security Standards\\n\\n      - **Input Validation**: Validate all user inputs\\n      - **File Safety**: Check files for potential security issues\\n      - **Permission Checks**: Verify file access permissions\\n      - **Rate Limiting**: Prevent abuse of API endpoints\\n\\n      ## Validation Requirements\\n\\n      - All code must have appropriate type annotations\\n      - All public APIs must have documentation comments\\n      - All code must have corresponding tests\\n      - All UI components must support internationalization\\n      - Security checks must be applied to all file operations\\n\\nexamples:\\n  - input: |\\n      # What are the requirements for TypeScript code in this project?\\n    output: |\\n      TypeScript code in this project must:\\n      1. Follow the code style defined in biome.json\\n      2. Use strong typing for all functions, variables, and classes\\n      3. Include JSDoc comments for public APIs\\n      4. Be accompanied by comprehensive tests\\n      5. Follow the module organization pattern (cli, core, config, etc.)\\n      6. Implement proper error handling\\n\\n  - input: |\\n      # How should I structure a new feature?\\n    output: |\\n      When adding a new feature:\\n\\n      1. Identify the appropriate module (cli, core, config, etc.)\\n      2. Create new files following the existing naming patterns\\n      3. Implement the feature with proper typing and documentation\\n      4. Add comprehensive tests in the corresponding test directory\\n      5. Update any relevant documentation\\n      6. Ensure the feature follows security standards if handling files or user input\\n\\nmetadata:\\n  priority: high\\n  version: 1.0\\n  tags:\\n    - standards\\n    - code-quality\\n    - organization\\n</rule>\\n\\n<rule>\\nname: repomix_context_extraction\\ndescription: Guide for using repomix to extract focused context from repositories\\nfilters:\\n  - type: message\\n    pattern: \\\"(?i)(extract context|code context|repository context|repomix extract|code explanation|understand code)\\\"\\n  - type: context\\n    pattern: \\\"explain code|understand repository|extract code|code extraction\\\"\\n\\nactions:\\n  - type: suggest\\n    message: |\\n      # Repomix Context Extraction Guide\\n\\n      Repomix is a powerful tool for extracting and summarizing code repositories for LLM consumption. This guide focuses on using repomix to extract only the necessary context to explain how specific functionality works.\\n\\n      ## Core Principles for Effective Context Extraction\\n\\n      - **Focused Selection**: Only include files/folders directly relevant to the functionality\\n      - **Complete Understanding**: Ensure all dependencies are included for a complete explanation\\n      - **XML Output**: Use XML format for structured representation of code\\n      - **Minimal Context**: Avoid including unnecessary files that add noise\\n      - **Consistent Ignore Patterns**: Use standard ignore patterns for common build artifacts and dependencies\\n\\n      ## Command Structure\\n\\n      ```bash\\n      repomix extract /path/to/repository --style xml --include \\\"path/to/relevant/files/**\\\" --exclude \\\"tests/**\\\" --ignore \\\"**/node_modules,**/dist,**/build\\\" --output output.xml\\n      ```\\n\\n      ## Key Parameters\\n\\n      - `--style xml`: Output in XML format for structured representation\\n      - `--include`: Specify patterns for files to include (supports glob patterns)\\n      - `--exclude`: Specify patterns for files to exclude\\n      - `--ignore`: Specify patterns for files to ignore (build artifacts, dependencies)\\n      - `--output`: Specify the output file\\n      - `--output-show-line-numbers`: Include line numbers in the output (helpful for reference)\\n      - `--max-tokens`: Limit the total token count (optional)\\n      - `--depth`: Control the depth of directory traversal (optional)\\n\\n      ## Standard Ignore Patterns\\n\\n      Always include these standard ignore patterns to avoid including unnecessary files:\\n\\n      ```bash\\n      --ignore \\\"**/uv.lock,**/package-lock.json,**/.env,**/Cargo.lock,**/node_modules,**/target,**/dist,**/build,**/output.txt,**/yarn.lock\\\"\\n      ```\\n\\n      ## Best Practices\\n\\n      ### 1. Identify Core Components\\n\\n      Before extraction, identify the core components needed to explain the functionality:\\n\\n      - Entry point files\\n      - Core implementation files\\n      - Essential utility functions\\n      - Type definitions and interfaces\\n      - Configuration files directly related to the functionality\\n\\n      ### 2. Use Precise Include Patterns\\n\\n      ```bash\\n      # Example: Extract authentication system\\n      repomix extract ./repo --style xml --include \\\"src/auth/**\\\" --include \\\"src/models/User.js\\\" --include \\\"src/config/auth.js\\\" --ignore \\\"**/node_modules,**/dist,**/build,**/package-lock.json\\\" --output-show-line-numbers --output auth-context.xml\\n      ```\\n\\n      ### 3. Exclude Unnecessary Files\\n\\n      ```bash\\n      # Exclude tests, documentation, and build artifacts\\n      repomix extract ./repo --style xml --exclude \\\"**/*.test.js\\\" --exclude \\\"**/*.spec.js\\\" --exclude \\\"docs/**\\\" --ignore \\\"**/uv.lock,**/package-lock.json,**/.env,**/Cargo.lock,**/node_modules,**/target,**/dist,**/build,**/output.txt,**/yarn.lock\\\" --output-show-line-numbers --output clean-context.xml\\n      ```\\n\\n      ### 4. Combine With Search When Needed\\n\\n      For complex functionality spanning multiple directories:\\n\\n      ```bash\\n      # First find relevant files\\n      find ./repo -type f -name \\\"*.js\\\" | grep -E \\\"auth|user|permission\\\" > relevant_files.txt\\n\\n      # Then use the list with repomix\\n      cat relevant_files.txt | xargs -I{} echo \\\"--include {}\\\" | xargs repomix extract ./repo --style xml --ignore \\\"**/node_modules,**/dist,**/build\\\" --output-show-line-numbers --output auth-system.xml\\n      ```\\n\\n      ### 5. Focus on Interfaces Over Implementation Details\\n\\n      When explaining how something works, prioritize interface files and core logic over detailed implementations:\\n\\n      ```bash\\n      repomix extract ./repo --style xml --include \\\"src/api/**/*.interface.ts\\\" --include \\\"src/core/**/*.ts\\\" --exclude \\\"**/*.impl.ts\\\" --ignore \\\"**/uv.lock,**/package-lock.json,**/.env,**/Cargo.lock,**/node_modules,**/target,**/dist,**/build,**/output.txt,**/yarn.lock\\\" --output-show-line-numbers --output system-overview.xml\\n      ```\\n\\n      ## Example Scenarios\\n\\n      ### Extracting a Feature Implementation\\n\\n      ```bash\\n      # Extract everything related to the search feature\\n      repomix extract ./repo --style xml --include \\\"src/**/search/**\\\" --include \\\"src/models/SearchIndex.js\\\" --include \\\"src/utils/searchHelpers.js\\\" --ignore \\\"**/uv.lock,**/package-lock.json,**/.env,**/Cargo.lock,**/node_modules,**/target,**/dist,**/build,**/output.txt,**/yarn.lock\\\" --output-show-line-numbers --output search-feature.xml\\n      ```\\n\\n      ### Extracting an API Endpoint Flow\\n\\n      ```bash\\n      # Extract files related to the user registration flow\\n      repomix extract ./repo --style xml --include \\\"src/controllers/UserController.js\\\" --include \\\"src/services/UserService.js\\\" --include \\\"src/models/User.js\\\" --include \\\"src/validators/userValidation.js\\\" --ignore \\\"**/node_modules,**/dist,**/build\\\" --output-show-line-numbers --output user-registration.xml\\n      ```\\n\\n      ### Extracting Architecture Overview\\n\\n      ```bash\\n      # Extract core architectural files\\n      repomix extract ./repo --style xml --include \\\"src/core/**/*.ts\\\" --include \\\"src/types/**/*.ts\\\" --include \\\"src/config/**/*.ts\\\" --exclude \\\"**/*.test.ts\\\" --ignore \\\"**/uv.lock,**/package-lock.json,**/.env,**/Cargo.lock,**/node_modules,**/target,**/dist,**/build,**/output.txt,**/yarn.lock\\\" --output-show-line-numbers --output architecture.xml\\n      ```\\n\\n      ### Quick Repository Summary\\n\\n      ```bash\\n      # Generate a quick summary of the entire repository\\n      npx repomix --style xml --output-show-line-numbers --output output.txt --ignore \\\"**/uv.lock,**/package-lock.json,**/.env,**/Cargo.lock,**/node_modules,**/target,**/dist,**/build,**/output.txt,**/yarn.lock\\\"\\n      ```\\n\\nexamples:\\n  - input: |\\n      How can I use repomix to extract just the authentication system from my repository?\\n    output: |\\n      To extract just the authentication system from your repository using repomix with XML output:\\n\\n      ```bash\\n      repomix extract ./your-repo --style xml \\\\\\n        --include \\\"src/**/auth/**\\\" \\\\\\n        --include \\\"src/**/user/**\\\" \\\\\\n        --include \\\"src/models/User.*\\\" \\\\\\n        --include \\\"src/middleware/auth.*\\\" \\\\\\n        --include \\\"src/config/auth.*\\\" \\\\\\n        --exclude \\\"**/*.test.*\\\" \\\\\\n        --exclude \\\"**/*.spec.*\\\" \\\\\\n        --ignore \\\"**/uv.lock,**/package-lock.json,**/.env,**/Cargo.lock,**/node_modules,**/target,**/dist,**/build,**/output.txt,**/yarn.lock\\\" \\\\\\n        --output-show-line-numbers \\\\\\n        --output auth-system.xml\\n      ```\\n\\n      This command:\\n      1. Targets all auth-related files and directories\\n      2. Includes user-related files that are typically part of authentication\\n      3. Includes specific model, middleware, and config files related to auth\\n      4. Excludes test files to keep the output focused\\n      5. Ignores common build artifacts and dependencies\\n      6. Includes line numbers for easier reference\\n      7. Outputs in XML format for structured representation\\n\\n      If you need to be even more selective, first identify the exact files involved:\\n\\n      ```bash\\n      find ./your-repo -type f -name \\\"*.js\\\" | grep -E \\\"auth|login|register|user|permission|role\\\" | less\\n      ```\\n\\n      Then use those specific files in your include patterns.\\n\\n  - input: |\\n      I need to extract context to explain how the data processing pipeline works in my repo.\\n    output: |\\n      To extract context that explains how a data processing pipeline works:\\n\\n      ```bash\\n      repomix extract ./your-repo --style xml \\\\\\n        --include \\\"src/**/pipeline/**\\\" \\\\\\n        --include \\\"src/**/processors/**\\\" \\\\\\n        --include \\\"src/models/DataModel.*\\\" \\\\\\n        --include \\\"src/utils/dataUtils.*\\\" \\\\\\n        --include \\\"src/config/pipeline.*\\\" \\\\\\n        --include \\\"src/types/data.*\\\" \\\\\\n        --exclude \\\"**/*.test.*\\\" \\\\\\n        --exclude \\\"**/mock*\\\" \\\\\\n        --ignore \\\"**/uv.lock,**/package-lock.json,**/.env,**/Cargo.lock,**/node_modules,**/target,**/dist,**/build,**/output.txt,**/yarn.lock\\\" \\\\\\n        --output-show-line-numbers \\\\\\n        --output data-pipeline.xml\\n      ```\\n\\n      This approach:\\n      1. Focuses on pipeline and processor directories\\n      2. Includes relevant data models and utilities\\n      3. Includes pipeline configuration\\n      4. Includes type definitions for data structures\\n      5. Excludes tests and mock data\\n      6. Ignores common build artifacts and dependencies\\n      7. Includes line numbers for easier reference\\n\\n      For a more comprehensive understanding, you might want to include the entry points:\\n\\n      ```bash\\n      repomix extract ./your-repo --style xml \\\\\\n        --include \\\"src/index.*\\\" \\\\\\n        --include \\\"src/**/pipeline/**\\\" \\\\\\n        --include \\\"src/**/processors/**\\\" \\\\\\n        --include \\\"src/models/DataModel.*\\\" \\\\\\n        --include \\\"src/utils/dataUtils.*\\\" \\\\\\n        --include \\\"src/config/pipeline.*\\\" \\\\\\n        --include \\\"src/types/data.*\\\" \\\\\\n        --exclude \\\"**/*.test.*\\\" \\\\\\n        --exclude \\\"**/mock*\\\" \\\\\\n        --ignore \\\"**/uv.lock,**/package-lock.json,**/.env,**/Cargo.lock,**/node_modules,**/target,**/dist,**/build,**/output.txt,**/yarn.lock\\\" \\\\\\n        --output-show-line-numbers \\\\\\n        --output data-pipeline-with-entry.xml\\n      ```\\n\\n      This ensures you capture how the pipeline is initialized and triggered.\\n\\n  - input: |\\n      How do I create a quick summary of my entire repository for an LLM?\\n    output: |\\n      For a quick summary of your entire repository optimized for LLM consumption:\\n\\n      ```bash\\n      npx repomix --style xml --output-show-line-numbers --output output.txt --ignore \\\"**/uv.lock,**/package-lock.json,**/.env,**/Cargo.lock,**/node_modules,**/target,**/dist,**/build,**/output.txt,**/yarn.lock\\\"\\n      ```\\n\\n      This command:\\n      1. Uses repomix to process your repository\\n      2. Outputs in XML format for structured representation\\n      3. Includes line numbers for easier reference\\n      4. Ignores common build artifacts and dependencies\\n      5. Creates an output.txt file with the repository summary\\n\\n      If you want to focus on specific aspects while still getting a broad overview:\\n\\n      ```bash\\n      npx repomix --style xml --include \\\"src/**/*.ts\\\" --include \\\"src/**/*.js\\\" --exclude \\\"**/*.test.*\\\" --ignore \\\"**/uv.lock,**/package-lock.json,**/.env,**/Cargo.lock,**/node_modules,**/target,**/dist,**/build,**/output.txt,**/yarn.lock\\\" --output-show-line-numbers --output repo-summary.xml\\n      ```\\n\\n      This will include all TypeScript and JavaScript files while excluding tests and common artifacts.\\n\\nmetadata:\\n  priority: high\\n  version: 1.0\\n  tags:\\n    - context-extraction\\n    - code-understanding\\n    - repomix-usage\\n</rule>\\n\"}], \"valid_rule_count\": 2}"}],"isError":false}
```

### get_static_cursor_rule

> Request:

```json
{"method":"tools/call","params":{"name":"get_static_cursor_rule","arguments":{"rule_name":"tree"},"_meta":{"progressToken":1}}}
```

> Response:

```json
{"content":[{"type":"text","text":"{\"rule_name\": \"tree.mdc.md\", \"content\": \"---\\ndescription: Display repository structure\\nglobs: *\\nalwaysApply: false\\n---\\nWhen the user asks about the project layout, structure, or organization, run the command `tree -L 7 -I \\\"*.pyc|__pycache__|.git|.pytest_cache|.ruff_cache|.mypy_cache|.coverage|htmlcov|.venv|.env|*.egg-info|build|dist|node_modules|.DS_Store|images\\\"` to help understand and visualize the repository structure.\\n\"}"}],"isError":false}
```


### save_cursor_rule

```json
{"method":"tools/call","params":{"name":"save_cursor_rule","arguments":{"rule_name":"bossjones","rule_content":"# Python Best Practices\\n\\nWhen writing Python code, follow these guidelines:\\n\\n1. Use type hints\\n2. Write docstrings\\n3. Follow PEP 8"},"_meta":{"progressToken":3}}}
```

```json
{"content":[{"type":"text","text":"{\"operations\": [{\"type\": \"create_directory\", \"path\": \"hack/drafts/cursor_rules\", \"options\": {\"parents\": true, \"exist_ok\": true}}, {\"type\": \"write_file\", \"path\": \"hack/drafts/cursor_rules/bossjones.mdc.md\", \"content\": \"# Python Best Practices\\\\n\\\\nWhen writing Python code, follow these guidelines:\\\\n\\\\n1. Use type hints\\\\n2. Write docstrings\\\\n3. Follow PEP 8\", \"options\": {\"mode\": \"w\"}}], \"message\": \"Instructions to save cursor rule to hack/drafts/cursor_rules/bossjones.mdc.md\"}"}],"isError":false}
```

### recommend_cursor_rules

```json
{"method":"tools/call","params":{"name":"recommend_cursor_rules","arguments":{"repo_summary":"fastapi fastapi fastapi fastapi fastapi fastapi fastapi"},"_meta":{"progressToken":5}}}
```

```json
{"content":[{"type":"text","text":"{\"name\": \"code-documentation\", \"description\": \"Standards for code documentation and comments\", \"reason\": \"Improve overall code documentation\"}"},{"type":"text","text":"{\"name\": \"error-handling\", \"description\": \"Best practices for error handling and logging\", \"reason\": \"Enhance application reliability with proper error handling\"}"},{"type":"text","text":"{\"name\": \"fastapi-best-practices\", \"description\": \"Best practices for FastAPI development\", \"reason\": \"Repository uses FastAPI framework\"}"},{"type":"text","text":"{\"name\": \"fastapi-security\", \"description\": \"Security considerations for FastAPI applications\", \"reason\": \"Ensure secure API development with FastAPI\"}"},{"type":"text","text":"{\"name\": \"fastapi-testing\", \"description\": \"Testing strategies for FastAPI endpoints\", \"reason\": \"Help with writing comprehensive tests for FastAPI endpoints\"}"},{"type":"text","text":"{\"name\": \"api-security\", \"description\": \"Security considerations for API development\", \"reason\": \"Repository implements APIs\"}"},{"type":"text","text":"{\"name\": \"api-documentation\", \"description\": \"Best practices for API documentation\", \"reason\": \"Improve API documentation\"}"}],"isError":false}
```


### prep_workspace

```
{"method":"tools/call","params":{"name":"prep_workspace","arguments":{},"_meta":{"progressToken":6}}}
```

```
{"content":[{"type":"text","text":"{\"status\": \"success\", \"message\": \"\\nTo prepare the workspace for cursor rules, the following steps are needed:\\n\\n1. Create the cursor rules directory structure, this should be relative to the repo root eg ./hack/drafts/cursor_rules:\\n   mkdir -p ./hack/drafts/cursor_rules .cursor/rules || true\\n\\n2. Ensure the .cursor/rules directory exists for deployment:\\n   mkdir -p .cursor/rules\\n   mkdir -p ./hack/drafts/cursor_rules\\n\\n3. Check if Makefile exists with an update-cursor-rules task:\\n   The update-cursor-rules task should copy files from hack/drafts/cursor_rules to .cursor/rules. This command updates Cursor editor rules by copying rule definitions from a drafts directory into the Cursor configuration folder. It first creates a .cursor/rules directory if it doesn't exist. Then it finds all Markdown (.md) files in the hack/drafts/cursor_rules directory (excluding any README files), copies them to the .cursor/rules directory, and preserves their filenames without the .md extension. The comment notes that Cursor doesn't support generating .mdc files directly through the Composer Agent at the time this was written\\n\\n\\n4. Update .dockerignore to exclude the cursor rules drafts directory:\\n   Add 'hack/drafts/cursor_rules' to .dockerignore if it exists\\n\\n5. Write the following mandatory cursor rule files to the client repo's cursor rules stage directory one at a time, using the get_static_cursor_rules function to retrieve each file. when saving the file ensure the file has extension .mdc.md, eg tree.mdc becomes tree.mdc.md:\\n   - tree.mdc: A rule for displaying repository structure\\n   - repo_analyzer.mdc: A rule for analyzing repository structure and locating code definitions\\n   - notify.mdc: A rule for notification at the end of tasks\\n   - repomix.mdc: A rule for repository summarization and packaging for LLM consumption\\n   - cursor_rules_location.mdc: A rule for locating the cursor rules directory and how to write them\\n\\n6. Update the client repo's .cursor/mcp.json file to include new entries if they don't already exist:\\n   Ensure the .cursor/mcp.json file contains entries for prompt_library and sequentialthinking:\\n   ```json\\n   {\\n     \\\"prompt_library\\\": {\\n       \\\"command\\\": \\\"uv\\\",\\n       \\\"args\\\": [\\n         \\\"run\\\",\\n         \\\"--with\\\",\\n         \\\"mcp[cli]\\\",\\n         \\\"mcp\\\",\\n         \\\"run\\\",\\n         \\\"${PWD}/src/codegen_lab/prompt_library.py\\\"\\n       ]\\n     },\\n     \\\"sequentialthinking\\\": {\\n       \\\"command\\\": \\\"npx\\\",\\n       \\\"args\\\": [\\n         \\\"-y\\\",\\n         \\\"@modelcontextprotocol/server-sequential-thinking\\\"\\n       ]\\n     }\\n   }\\n   ```\\n   Note: The path in the prompt_library entry should be adjusted to use the actual project path (PWD) instead of hardcoded paths.\\n\", \"directory_exists\": true, \"directory_path\": \"./hack/drafts/cursor_rules\", \"mkdir_command\": \"mkdir -p ./hack/drafts/cursor_rules .cursor/rules || true\", \"directory_structure\": \"Directory structure to create at ./hack/drafts/cursor_rules\", \"workspace_prepared\": false, \"workspace_result\": {\"status\": \"success\", \"message\": \"Instructions provided for workspace preparation at ./hack/drafts/cursor_rules\", \"directory_exists\": true, \"directory_path\": \"./hack/drafts/cursor_rules\", \"mkdir_command\": \"mkdir -p ./hack/drafts/cursor_rules .cursor/rules || true\"}}"}],"isError":false}
```

### create_cursor_rule_files

```
{"method":"tools/call","params":{"name":"create_cursor_rule_files","arguments":{"rule_names":["bossjones"]},"_meta":{"progressToken":7}}}
```

```
{"content":[{"type":"text","text":"{\"success\": true, \"operations\": [{\"type\": \"create_directory\", \"path\": \"hack/drafts/cursor_rules\", \"options\": {\"parents\": true, \"exist_ok\": true}}, {\"type\": \"write_file\", \"path\": \"hack/drafts/cursor_rules/bossjones.mdc.md\", \"content\": \"\", \"options\": {\"mode\": \"w\"}}], \"created_files\": [\"bossjones.mdc.md\"], \"touch_command\": \"touch hack/drafts/cursor_rules/bossjones.mdc.md\", \"next_steps\": \"\\nNext steps:\\n1. Write content to each file sequentially\\n2. Deploy the rules using 'make update-cursor-rules'\\n3. Verify the rules are correctly deployed to .cursor/rules\\n\", \"message\": \"Instructions to create 1 empty cursor rule files in hack/drafts/cursor_rules. \\nNext steps:\\n1. Write content to each file sequentially\\n2. Deploy the rules using 'make update-cursor-rules'\\n3. Verify the rules are correctly deployed to .cursor/rules\\n\"}"}],"isError":false}
```


### ensure_makefile_task


```
{"method":"tools/call","params":{"name":"ensure_makefile_task","arguments":{"makefile_path":"Makefile"},"_meta":{"progressToken":8}}}
```

```
{"content":[{"type":"text","text":"{\"operations\": [{\"type\": \"check_file_exists\", \"path\": \"Makefile\"}, {\"type\": \"read_file\", \"path\": \"Makefile\", \"options\": {\"encoding\": \"utf-8\"}}], \"requires_result\": true, \"message\": \"Instructions to check Makefile and update if needed\", \"update_task_content\": \"\\n# Cursor Rules\\n.PHONY: update-cursor-rules\\nupdate-cursor-rules:  ## Update cursor rules from prompts/drafts/cursor_rules\\n\\t# Create .cursor/rules directory if it doesn't exist.\\n\\t# Note: at the time of writing, cursor does not support generating .mdc files via Composer Agent.s\\n\\tmkdir -p .cursor/rules || true\\n\\t# Copy files from hack/prompts/drafts/cursor_rules to .cursor/rules and change extension to .mdc\\n\\t# Exclude README.md files from being copied\\n\\tfind hack/drafts/cursor_rules -type f -name \\\"*.md\\\" ! -name \\\"README.md\\\" -exec sh -c 'for file; do target=$${file%.md}; cp -a \\\"$$file\\\" \\\".cursor/rules/$$(basename \\\"$$target\\\")\\\"; done' sh {} +\\n\", \"next_steps\": \"After applying these operations, you'll need to check if the Makefile exists and contains the update-cursor-rules task, then update or create it accordingly.\"}"}],"isError":false}
```
